---
title: A case where a spindly two-layer linear network decisively outperforms any
  neural network with a fully connected input layer
abstract: 'It was conjectured that any neural network of any structure and arbitrary
  differentiable transfer functions at the nodes cannot learn the following problem
  sample efficiently when trained with gradient descent: The instances are the rows
  of a $d$-dimensional Hadamard matrix and the target is one of the features, i.e.
  very sparse. We essentially prove this conjecture: We show that after receiving
  a random training set of size $k < d$, the expected squared loss is still $1-\frac{k}{(d-1)}$.
  The only requirement needed is that the input layer is fully connected and the initial
  weight vectors of the input nodes are chosen from a rotation invariant distribution.
  Surprisingly the same type of problem can be solved drastically more efficient by
  a simple 2-layer linear neural network in which the $d$ inputs are connected to
  the output node by chains of length 2 (Now the input layer has only one edge per
  input). When such a network is trained by gradient descent, then it has been shown
  that its expected squared loss is $\frac{\log d}{k}$. Our lower bounds essentially
  show that a sparse input layer is needed to sample efficiently learn sparse targets
  with gradient descent.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: warmuth21a
month: 0
tex_title: A case where a spindly two-layer linear network decisively outperforms
  any neural network with a fully connected input layer
firstpage: 1214
lastpage: 1236
page: 1214-1236
order: 1214
cycles: false
bibtex_author: Warmuth, Manfred K. and Kot{\l}owski, Wojciech and Amid, Ehsan
author:
- given: Manfred K.
  family: Warmuth
- given: Wojciech
  family: KotÅ‚owski
- given: Ehsan
  family: Amid
date: 2021-03-01
address: 
container-title: Proceedings of the 32nd International Conference on Algorithmic Learning
  Theory
volume: '132'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 1
pdf: http://proceedings.mlr.press/v132/warmuth21a/warmuth21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
