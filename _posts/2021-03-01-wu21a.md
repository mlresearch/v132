---
title: Non-uniform Consistency of Online Learning with Random Sampling
abstract: " We study the problem of online learning a hypothesis class and a given
  binary 0-1 loss function, using instances generated $i.i.d.$ by a given distribution.
  The goal of the online learner is to make only finitely many errors (loss 1) with
  probability $1$ in the infinite horizon. In the binary label case, we show that
  hypothesis classes are online learnable in the above sense if and only if the class
  is effectively countable. We extend the results to hypothesis classes where labels
  can be non-binary. Characterization of non-binary online learnable classes is more
  involved for general loss functions and is not captured fully by the countability
  condition even for the ternary label case. In the computational bounded setup, we
  compare our results with well known results in recursive function learning, showing
  that the class of all total computable functions is indeed learnable with computable
  online learners and randomized sampling. Finally, we also show that the finite error
  guarantee will not be affected even when independent noise is added to the label."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu21a
month: 0
tex_title: Non-uniform Consistency of Online Learning with Random Sampling
firstpage: 1265
lastpage: 1285
page: 1265-1285
order: 1265
cycles: false
bibtex_author: Wu, Changlong and Santhanam, Narayana
author:
- given: Changlong
  family: Wu
- given: Narayana
  family: Santhanam
date: 2021-03-01
address: 
container-title: Proceedings of the 32nd International Conference on Algorithmic Learning
  Theory
volume: '132'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 1
pdf: http://proceedings.mlr.press/v132/wu21a/wu21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
