---
title: Contrastive learning, multi-view redundancy, and linear models
abstract: Self-supervised learning is an empirically successful approach to unsupervised
  learning based on creating artificial supervised learning problems. A popular self-supervised
  approach to representation learning is contrastive learning, which leverages naturally
  occurring pairs of similar and dissimilar data points, or multiple views of the
  same data. This work provides a theoretical analysis of contrastive learning in
  the multi-view setting, where two views of each datum are available. The main result
  is that linear functions of the learned representations are nearly optimal on downstream
  prediction tasks whenever the two views provide redundant information about the
  label.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tosh21a
month: 0
tex_title: Contrastive learning, multi-view redundancy, and linear models
firstpage: 1179
lastpage: 1206
page: 1179-1206
order: 1179
cycles: false
bibtex_author: Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel
author:
- given: Christopher
  family: Tosh
- given: Akshay
  family: Krishnamurthy
- given: Daniel
  family: Hsu
date: 2021-03-01
address: 
container-title: Proceedings of the 32nd International Conference on Algorithmic Learning
  Theory
volume: '132'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 1
pdf: http://proceedings.mlr.press/v132/tosh21a/tosh21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
