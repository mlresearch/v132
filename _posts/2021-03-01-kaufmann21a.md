---
title: Adaptive Reward-Free Exploration
abstract: 'Reward-free exploration is a reinforcement learning setting recently studied
  by (Jin et al. 2020), who address it by running several algorithms with regret guarantees
  in parallel. In our work, we instead propose a more natural adaptive approach for
  reward-free exploration which directly reduces upper bounds on the maximum MDP estimation
  error. We show that, interestingly, our reward-free UCRL algorithm can be seen as
  a variant of an algorithm by Fiechter from 1994, originally proposed for a different
  objective that we call best-policy identification. We prove that RF-UCRL needs of
  order (SAH^4/\epsilon^2)(log(1/\delta) + S) episodes to output, with probability
  1-\delta, an \epsilon-approximation of the optimal policy for any reward function.
  This bound improves over existing sample complexity bounds in both the small \epsilon
  and the small \delta regimes. We further investigate the relative complexities of
  reward-free exploration and best policy identification. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kaufmann21a
month: 0
tex_title: Adaptive Reward-Free Exploration
firstpage: 865
lastpage: 891
page: 865-891
order: 865
cycles: false
bibtex_author: Kaufmann, Emilie and M{\'e}nard, Pierre and Darwiche Domingues, Omar
  and Jonsson, Anders and Leurent, Edouard and Valko, Michal
author:
- given: Emilie
  family: Kaufmann
- given: Pierre
  family: MÃ©nard
- given: Omar
  family: Darwiche Domingues
- given: Anders
  family: Jonsson
- given: Edouard
  family: Leurent
- given: Michal
  family: Valko
date: 2021-03-01
address: 
container-title: Proceedings of the 32nd International Conference on Algorithmic Learning
  Theory
volume: '132'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 1
pdf: http://proceedings.mlr.press/v132/kaufmann21a/kaufmann21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
