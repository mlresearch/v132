---
title: Subspace Embeddings under Nonlinear Transformations
abstract: 'We consider low-distortion embeddings for subspaces under \emph{entrywise
  nonlinear transformations}. In particular we seek embeddings that preserve the norm
  of all vectors in a space $S = \{y: y = f(x)\text{ for }x \in Z\}$, where $Z$ is
  a $k$-dimensional subspace of $\R^n$ and $f(x)$ is a nonlinear activation function
  applied entrywise to $x$. When $f$ is the identity, and so $S$ is just a $k$-dimensional
  subspace, it is known that, with high probability, a random embedding into $O(k/\epsilon^2)$
  dimensions preserves the norm of all $y \in S$ up to $(1\pm \epsilon)$ relative
  error. Such embeddings are known as \emph{subspace embeddings}, and have found widespread
  use in compressed sensing and approximation algorithms.% for regression, PCA, and
  many other problems. We give the first low-distortion embeddings for a wide class
  of nonlinear functions $f$. In particular, we give additive $\epsilon$ error embeddings
  into $O(\frac{k\log (n/\epsilon)}{\epsilon^2})$ dimensions for a class of nonlinearities
  that includes the popular Sigmoid SoftPlus, and Gaussian functions. We strengthen
  this result to give relative error embeddings under some further restrictions, which
  are satisfied e.g., by the Tanh, SoftSign, Exponential Linear Unit, and many other
  ‘soft’ step functions and rectifying units. Understanding embeddings for subspaces
  under nonlinear transformations is a key step towards extending random sketching
  and compressing sensing techniques for linear problems to nonlinear ones. We discuss
  example applications of our results to improved bounds for compressed sensing via
  generative neural networks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gajjar21a
month: 0
tex_title: Subspace Embeddings under Nonlinear Transformations
firstpage: 656
lastpage: 672
page: 656-672
order: 656
cycles: false
bibtex_author: Gajjar, Aarshvi and Musco, Cameron
author:
- given: Aarshvi
  family: Gajjar
- given: Cameron
  family: Musco
date: 2021-03-01
address: 
container-title: Proceedings of the 32nd International Conference on Algorithmic Learning
  Theory
volume: '132'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 1
pdf: http://proceedings.mlr.press/v132/gajjar21a/gajjar21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
